{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1a7ac52",
      "metadata": {},
      "source": [
        "### Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace566c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastmcp import Client\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery\n",
        "\n",
        "from langsmith import traceable, get_current_run_tree\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "from langchain_core.messages import AIMessage, ToolMessage, convert_to_openai_messages\n",
        "\n",
        "from jinja2 import Template\n",
        "from typing import Literal, Dict, Any, Annotated, List, Optional\n",
        "from IPython.display import Image, display\n",
        "from operator import add\n",
        "from openai import OpenAI\n",
        "\n",
        "from utils.utils import format_ai_message\n",
        "\n",
        "import openai\n",
        "\n",
        "import random\n",
        "import ast\n",
        "import inspect\n",
        "import instructor\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4179b83",
      "metadata": {},
      "source": [
        "### List available tools in MCP servers running on http://localhost:8001/mcp and http://localhost:8002/mcp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb400596",
      "metadata": {},
      "outputs": [],
      "source": [
        "client = Client(\"http://localhost:8001/mcp\")\n",
        "\n",
        "async with client:\n",
        "\n",
        "    tools = await client.list_tools()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c06152d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8a0494",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"======NAME=======\")\n",
        "print(tools[0].name)\n",
        "print(\"======DESCRIPTION=======\")\n",
        "print(tools[0].description)\n",
        "print(\"======INPUT SCHEMA=======\")\n",
        "print(tools[0].inputSchema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7c8492",
      "metadata": {},
      "outputs": [],
      "source": [
        "client = Client(\"http://localhost:8002/mcp\")\n",
        "\n",
        "async with client:\n",
        "\n",
        "    tools = await client.list_tools()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f08ba3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1fab3b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"======NAME=======\")\n",
        "print(tools[0].name)\n",
        "print(\"======DESCRIPTION=======\")\n",
        "print(tools[0].description)\n",
        "print(\"======INPUT SCHEMA=======\")\n",
        "print(tools[0].inputSchema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc390fd",
      "metadata": {},
      "source": [
        "### Execute a tool on one of the running MCP Servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e60c4ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "client = Client(\"http://localhost:8001/mcp\")\n",
        "\n",
        "async with client:\n",
        "    # List available resources\n",
        "    result = await client.call_tool(\"get_formatted_item_context\", {\"query\": \"earphones\", \"top_k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd769c60",
      "metadata": {},
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44dcb619",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(result.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d144a3d5",
      "metadata": {},
      "source": [
        "### A function to extract tool definitions of all available tools in provided MCP Servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a928a9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_docstring_params(docstring: str) -> Dict[str, str]:\n",
        "    \"\"\"Extract parameter descriptions from docstring (handles both Args: and Parameters: formats).\"\"\"\n",
        "    params = {}\n",
        "    lines = docstring.split('\\n')\n",
        "    in_params = False\n",
        "    current_param = None\n",
        "    \n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        \n",
        "        # Check for parameter section start\n",
        "        if stripped in ['Args:', 'Arguments:', 'Parameters:', 'Params:']:\n",
        "            in_params = True\n",
        "            current_param = None\n",
        "        elif stripped.startswith('Returns:') or stripped.startswith('Raises:'):\n",
        "            in_params = False\n",
        "        elif in_params:\n",
        "            # Parse parameter line (handles \"param: desc\" and \"- param: desc\" formats)\n",
        "            if ':' in stripped and (stripped[0].isalpha() or stripped.startswith(('-', '*'))):\n",
        "                param_name = stripped.lstrip('- *').split(':')[0].strip()\n",
        "                param_desc = ':'.join(stripped.lstrip('- *').split(':')[1:]).strip()\n",
        "                params[param_name] = param_desc\n",
        "                current_param = param_name\n",
        "            elif current_param and stripped:\n",
        "                # Continuation of previous parameter description\n",
        "                params[current_param] += ' ' + stripped\n",
        "    \n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a107d3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def get_tool_descriptions_from_mcp_servers(mcp_servers: list[str]) -> list[dict]:\n",
        "\n",
        "    tool_descriptions = []\n",
        "\n",
        "    for server in mcp_servers:\n",
        "\n",
        "        client = Client(server)\n",
        "\n",
        "        async with client:\n",
        "\n",
        "            tools = await client.list_tools()\n",
        "\n",
        "            for tool in tools:\n",
        "                \n",
        "                result = {\n",
        "                    \"name\": \"\",\n",
        "                    \"description\": \"\",\n",
        "                    \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
        "                    \"required\": [],\n",
        "                    \"returns\": {\"type\": \"string\", \"description\": \"\"},\n",
        "                    \"server\": server\n",
        "                }\n",
        "\n",
        "                result[\"name\"] = tool.name\n",
        "                result[\"required\"] = tool.inputSchema.get(\"required\", [])\n",
        "\n",
        "                ## Get Description\n",
        "\n",
        "                description = tool.description.split(\"\\n\\n\")[0]\n",
        "                result[\"description\"] = description\n",
        "\n",
        "\n",
        "                ## Get Returns\n",
        "\n",
        "                returns = tool.description.split(\"Returns:\")[1].strip()\n",
        "                result[\"returns\"][\"description\"] = returns\n",
        "\n",
        "                ## Get parameters\n",
        "\n",
        "                property_descriptions = parse_docstring_params(tool.description)\n",
        "                properties = tool.inputSchema.get(\"properties\", {})\n",
        "                for key, value in properties.items():\n",
        "                    properties[key][\"description\"] = property_descriptions.get(key, \"\")\n",
        "\n",
        "                result[\"parameters\"][\"properties\"] = properties\n",
        "\n",
        "                tool_descriptions.append(result)\n",
        "\n",
        "    return tool_descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7c0203",
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_servers = [\"http://localhost:8001/mcp\", \"http://localhost:8002/mcp\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89fb22b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_descriptions = await get_tool_descriptions_from_mcp_servers(mcp_servers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f4df89",
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38cb090f",
      "metadata": {},
      "source": [
        "## Agent integration with tools exposed via MCP Servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ec6c59",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToolCall(BaseModel):\n",
        "    name: str\n",
        "    arguments: dict\n",
        "    server: str\n",
        "\n",
        "class RAGUsedContext(BaseModel):\n",
        "    id: str = Field(description=\"ID of the item used to answer the question.\")\n",
        "    description: str = Field(description=\"Short description of the item used to answer the question.\")\n",
        "\n",
        "class AgentResponse(BaseModel):\n",
        "    answer: str = Field(description=\"Answer to the question.\")\n",
        "    references: list[RAGUsedContext] = Field(description=\"List of items used to answer the question.\")\n",
        "    final_answer: bool = False\n",
        "    tool_calls: List[ToolCall] = []\n",
        "\n",
        "class State(BaseModel):\n",
        "    messages: Annotated[List[Any], add] = []\n",
        "    question_relevant: bool = False\n",
        "    iteration: int = 0\n",
        "    answer: str = \"\"\n",
        "    available_tools: List[Dict[str, Any]] = []\n",
        "    tool_calls: List[ToolCall] = []\n",
        "    final_answer: bool = False\n",
        "    references: Annotated[List[RAGUsedContext], add] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fdbac2",
      "metadata": {},
      "outputs": [],
      "source": [
        "@traceable(\n",
        "    name=\"agent_node\",\n",
        "    run_type=\"llm\",\n",
        "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
        ")\n",
        "def agent_node(state: State) -> dict:\n",
        "\n",
        "   prompt_template =  \"\"\"You are a shopping assistant that can answer questions about the products in stock.\n",
        "\n",
        "You will be given a conversation history and a list of tools you can use to answer the latest query.\n",
        "\n",
        "<Available tools>\n",
        "{{ available_tools | tojson }}\n",
        "</Available tools>\n",
        "\n",
        "When making tool calls, use this exact format:\n",
        "{\n",
        "    \"name\": \"tool_name\",\n",
        "    \"arguments\": {\n",
        "        \"parameter1\": \"value1\",\n",
        "        \"parameter2\": \"value2\",\n",
        "    },\n",
        "    \"server\": \"server_name\"\n",
        "}\n",
        "\n",
        "CRITICAL: All parameters must go inside the \"arguments\" object, not at the top level of the tool call.\n",
        "\n",
        "Examples:\n",
        "- Get formatted item context:\n",
        "{\n",
        "    \"name\": \"get_formatted_item_context\",\n",
        "    \"arguments\": {\n",
        "        \"query\": \"Kool kids toys.\",\n",
        "        \"top_k\": 5\n",
        "    },\n",
        "    \"server\": \"http://localhost:8001/mcp\"\n",
        "}\n",
        "\n",
        "- Get formatted user reviews:\n",
        "    {\n",
        "        \"name\": \"get_formatted_reviews_context\",\n",
        "        \"arguments\": {\n",
        "            \"query\": \"Durable.\",\n",
        "            \"item_list\": [\"123\", \"456\"],\n",
        "            \"top_k\": 5\n",
        "        },\n",
        "        \"server\": \"http://localhost:8002/mcp\"\n",
        "    }\n",
        "\n",
        "CRITICAL RULES:\n",
        "- If tool_calls has values, final_answer MUST be false\n",
        "(You cannot call tools and exit the graph in the same response)\n",
        "- If final_answer is true, tool_calls MUST be []\n",
        "(You must wait for tool results before exiting the graph)\n",
        "- If you need tool results before answering, set:\n",
        "tool_calls=[...], final_answer=false\n",
        "- After receiving tool results, you can then set:\n",
        "tool_calls=[], final_answer=true\n",
        "- When suggesting tool calls, use names specificly provided in the available tools. Don't add any additional text to the names. You can only use get_formatted_item_context or get_formatted_reviews_context for tool names.\n",
        "\n",
        "Instructions:\n",
        "- You need to answer the question based on the outputs from the tools using the available tools only.\n",
        "- Do not suggest the same tool call more than once.\n",
        "- If the question can be decomposed into multiple sub-questions, suggest all of them.\n",
        "- If multipple tool calls can be used at once to answer the question, suggest all of them.\n",
        "- Do not explain your next steps in the answer, instead use tools to answer the question.\n",
        "- Never use word context and refer to it as the available products.\n",
        "- If using reviews data, make sure to match reviews to item IDs and summarise them instead of returning them as is, clearly split reviews from general item specification.\n",
        "- You should only answer questions about the products in stock. If the question is not about the products in stock, you should ask for clarification.\n",
        "- As an output you need to return the following:\n",
        "\n",
        "* answer: The answer to the question based on your current knowledge and the tool results.\n",
        "* references: The list of the indexes from the chunks returned from all tool calls that were used to answer the question. If more than one chunk was used to compile the answer from a single tool call, be sure to return all of them.\n",
        "* Each reference should have an id and a short description of the item based on the retrieved context.\n",
        "* final_answer: True if you have all the information needed to provide a complete answer, False otherwise.\n",
        "\n",
        "- The answer to the question should contain detailed information about the product and should be returned with detailed specification in bullet points.\n",
        "- The short description should have the name of the item.\n",
        "- If the user's request requires using a tool, set tool_calls with the appropriate function names and arguments.\n",
        "\"\"\"\n",
        "\n",
        "   template = Template(prompt_template)\n",
        "   \n",
        "   prompt = template.render(\n",
        "      available_tools=state.available_tools\n",
        "   )\n",
        "\n",
        "   messages = state.messages\n",
        "\n",
        "   conversation = []\n",
        "\n",
        "   for message in messages:\n",
        "        conversation.append(convert_to_openai_messages(message))\n",
        "\n",
        "   client = instructor.from_openai(OpenAI())\n",
        "\n",
        "   response, raw_response = client.chat.completions.create_with_completion(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        response_model=AgentResponse,\n",
        "        messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
        "        temperature=0.5,\n",
        "   )\n",
        "\n",
        "   ai_message = format_ai_message(response)\n",
        "\n",
        "   return {\n",
        "      \"messages\": [ai_message],\n",
        "      \"tool_calls\": response.tool_calls,\n",
        "      \"iteration\": state.iteration + 1,\n",
        "      \"answer\": response.answer,\n",
        "      \"final_answer\": response.final_answer,\n",
        "      \"references\": response.references\n",
        "   }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272adfa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tool_router(state: State) -> str:\n",
        "    \"\"\"Decide whether to continue or end\"\"\"\n",
        "    \n",
        "    if state.final_answer:\n",
        "        return \"end\"\n",
        "    elif state.iteration > 2:\n",
        "        return \"end\"\n",
        "    elif len(state.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    else:\n",
        "        return \"end\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9928a98",
      "metadata": {},
      "outputs": [],
      "source": [
        "class IntentRouterResponse(BaseModel):\n",
        "    question_relevant: bool\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac35614",
      "metadata": {},
      "outputs": [],
      "source": [
        "@traceable(\n",
        "    name=\"agent_node\",\n",
        "    run_type=\"llm\",\n",
        "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
        ")\n",
        "def intent_router_node(state: State):\n",
        "\n",
        "   prompt_template =  \"\"\"You are part of a shopping assistant that can answer questions about products in stock.\n",
        "\n",
        "Instructions:\n",
        "- You will be given a conversation history with the user.\n",
        "- If the latest user messgae is not relevant, return False in field \"question_relevant\" and set \"answer\" to explanation why it is not relevant.\n",
        "- If the latest user message is relevant, return True in field \"question_relevant\" and set \"answer\" to \"\".\n",
        "- You should only answer questions about the products in stock. If the question is not about the products in stock, you should ask for clarification.\n",
        "\"\"\"\n",
        "\n",
        "   template = Template(prompt_template)\n",
        "   \n",
        "   prompt = template.render()\n",
        "\n",
        "   messages = state.messages\n",
        "\n",
        "   conversation = []\n",
        "\n",
        "   for message in messages:\n",
        "        conversation.append(convert_to_openai_messages(message))\n",
        "\n",
        "   client = instructor.from_openai(OpenAI())\n",
        "\n",
        "   response, raw_response = client.chat.completions.create_with_completion(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        response_model=IntentRouterResponse,\n",
        "        messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
        "        temperature=0.5,\n",
        "   )\n",
        "\n",
        "   return {\n",
        "      \"question_relevant\": response.question_relevant,\n",
        "      \"answer\": response.answer\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0cac34",
      "metadata": {},
      "outputs": [],
      "source": [
        "def intent_router_conditional_edges(state: State):\n",
        "\n",
        "    if state.question_relevant:\n",
        "        return \"agent_node\"\n",
        "    else:\n",
        "        return \"end\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71eb640b",
      "metadata": {},
      "source": [
        "### Custom implementation of MCP Tool Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b9b24f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def mcp_tool_node(state: State) -> str:\n",
        "\n",
        "    tool_messages = []\n",
        "\n",
        "    for i, tc in enumerate(state.tool_calls):\n",
        "\n",
        "        client = Client(tc.server)\n",
        "\n",
        "        async with client:\n",
        "\n",
        "            result = await client.call_tool(tc.name, tc.arguments)\n",
        "\n",
        "            tool_message = ToolMessage(\n",
        "                content=result,\n",
        "                tool_call_id=f\"call_{i}\"\n",
        "            )\n",
        "\n",
        "            tool_messages.append(tool_message)\n",
        "\n",
        "    return {\n",
        "        \"messages\": tool_messages\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f795d8fa",
      "metadata": {},
      "source": [
        "## Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affa71bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "workflow = StateGraph(State)\n",
        "\n",
        "mcp_servers = [\"http://localhost:8001/mcp\", \"http://localhost:8002/mcp\"]\n",
        "tool_descriptions = await get_tool_descriptions_from_mcp_servers(mcp_servers)\n",
        "\n",
        "workflow.add_node(\"agent_node\", agent_node)\n",
        "workflow.add_node(\"mcp_tool_node\", mcp_tool_node)\n",
        "workflow.add_node(\"intent_router_node\", intent_router_node)\n",
        "\n",
        "workflow.add_edge(START, \"intent_router_node\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"intent_router_node\",\n",
        "    intent_router_conditional_edges,\n",
        "    {\n",
        "        \"agent_node\": \"agent_node\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent_node\",\n",
        "    tool_router,\n",
        "    {\n",
        "        \"tools\": \"mcp_tool_node\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"mcp_tool_node\", \"agent_node\")\n",
        "\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85408678",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2364611a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5ef18f",
      "metadata": {},
      "outputs": [],
      "source": [
        "state = {\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Can I get earphones for myself, a laptop bag for my wife and something cool for my kids?\"}],\n",
        "    \"available_tools\": tool_descriptions\n",
        "}\n",
        "config = {\"configurable\": {\"thread_id\": \"test000050\"}}\n",
        "\n",
        "async with AsyncPostgresSaver.from_conn_string(\"postgresql://langgraph_user:langgraph_password@localhost:5433/langgraph_db\") as checkpointer:\n",
        "\n",
        "    graph = workflow.compile(checkpointer=checkpointer)\n",
        "\n",
        "    async for chunk in graph.astream(\n",
        "        state,\n",
        "        config=config,\n",
        "        stream_mode=\"updates\"\n",
        "    ):\n",
        "        print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "343081d9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
